# Retail Pipeline ğŸš€

A modern **end-to-end data pipeline** for processing retail sales data from **Tier-2 & Tier-3 Indian cities**, powered by **Apache Spark**, **PostgreSQL**, and a lightweight **FastAPI service**.

This project automates the journey from **raw CSV retail transactions** â†’ **cleaned & aggregated data** â†’ **database storage** â†’ **REST API for insights**.

---

## âœ¨ Features
- **ETL with Apache Spark**: Cleans, transforms, and aggregates sales data.
- **Data Persistence**: Stores processed results in PostgreSQL.
- **API Layer**: FastAPI serves processed data for analytics & dashboards.
- **Volume Management**: Ensures repeatable Spark runs with atomic writes.
- **Health Checks**: PostgreSQL service monitored for readiness before load.

---

## ğŸ“‚ Project Structure
```bash
.
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py               # FastAPI app
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                # Dashboard application (e.g., Streamlit)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed_sales/       # Processed parquet data
â”‚   â””â”€â”€ indian_retail_tier2_tier3.csv  # Raw input dataset
â”œâ”€â”€ data_ingestion/
â”‚   â””â”€â”€ fetch_sales.py        # Script for data ingestion
â”œâ”€â”€ db/
â”‚   â””â”€â”€ load_to_db.py         # Loads processed parquet â†’ Postgres
â”œâ”€â”€ processing/
â”‚   â””â”€â”€ spark_etl.py          # Spark ETL job
â”œâ”€â”€ venv/                     # Python virtual environment (ignored in Docker)
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ .gitignore                # Git ignore rules
â”œâ”€â”€ accuracy.ipynb            # Model evaluation notebook
â”œâ”€â”€ initial_EDA.ipynb         # Exploratory Data Analysis notebook
â”œâ”€â”€ requirement.txt           # Python dependencies
```

---

## âš™ï¸ Tech Stack
- **Apache Spark 3.5** (via Bitnami image) â€“ Scalable ETL processing
- **PostgreSQL 15** â€“ Robust relational database
- **FastAPI** â€“ Modern async Python API

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Prerequisites
- Basic knowledge of Python & SQL (optional)

### 2ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/retail-pipeline.git
cd retail-pipeline
```

### 3ï¸âƒ£ Environment Setup
Create a `.env` file with database credentials:
```env
DB_USER=retail_user
DB_PASS=retail_pass
DB_NAME=retail_db
DB_PORT=5433   # default mapped port
```


### 4 Access the services
- **API** â†’ [http://localhost:8000](http://localhost:8000)
- **Postgres** â†’ Host: `localhost`, Port: `5433`

---

## ğŸ”„ Workflow
1. **Spark Job**
   - Reads raw CSV from `/app/data`
   - Cleans missing values & transforms schema
   - Aggregates sales per city & product
   - Writes processed parquet â†’ `/app/processed_sales/current`

2. **Loader**
   - Reads parquet files from shared volume
   - Loads aggregated data into PostgreSQL

3. **API**
   - Exposes endpoints to query processed data
   - Ready to integrate with BI dashboards or frontends

---

## ğŸ“Š Example Output
**Aggregated Sales (per city & product):**

| StoreCity | ProductID | Total_Sales | Avg_Sales |
|-----------|-----------|-------------|-----------|
| Delhi     | P123      | 150000.00   | 250.50    |
| Jaipur    | P456      |  78000.00   | 180.30    |

---

## ğŸ”§ Development Notes
- The pipeline uses **atomic writes** (`/processed_sales/current`) to avoid Spark overwrite errors.
- Spark job uses **temporary directories** for safe concurrent runs.
- To reset everything (including volumes):


---

## ğŸ“Œ Future Enhancements
- âœ… Add multi-tenant system (will be given api key and shop id to every shop)
- âœ… Expand API with filtering & analytics endpoints
- âœ… Add dashboard UI with React/Next.js
- âœ… Integrate monitoring (Prometheus + Grafana)
