# Retail Pipeline ğŸš€

An **end-to-end data pipeline** for processing retail sales data from **Tier-2 & Tier-3 Indian cities**.It automates the flow from **raw CSVs to a REST API**, powered by **Apache Spark**, **PostgreSQL**, and **FastAPI**.

---

## âœ¨ Workflow & Features

This project implements a classic ETL pattern to deliver analysis-ready data.
**Raw CSV**â†’ **Spark (Clean & Aggregate)** â†’ **PostgreSQL (Store)** â†’ **FastAPI (Serve)**

- **Scalable ETL**: Cleans and aggregates data efficiently with Apache Spark.
- **Reliable Storage**: Persists processed data in PostgreSQL.
- **Fast API Layer**: Exposes data for dashboards via FastAPI.
- **Atomic Writes**: Ensures safe, repeatable Spark job execution.
- **Health Checks**: Waits for the database to be ready before loading data.

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py               # FastAPI app
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                # Dashboard application (e.g., Streamlit)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed_sales/       # Processed parquet data
â”‚   â””â”€â”€ indian_retail_tier2_tier3.csv  # Raw input dataset
â”œâ”€â”€ data_ingestion/
â”‚   â””â”€â”€ fetch_sales.py        # Script for data ingestion
â”œâ”€â”€ db/
â”‚   â””â”€â”€ load_to_db.py         # Loads processed parquet â†’ Postgres
â”œâ”€â”€ processing/
â”‚   â””â”€â”€ spark_etl.py          # Spark ETL job
â”œâ”€â”€ venv/                     # Python virtual environment (ignored in Docker)
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ .gitignore                # Git ignore rules
â”œâ”€â”€ accuracy.ipynb            # Model evaluation notebook
â”œâ”€â”€ initial_EDA.ipynb         # Exploratory Data Analysis notebook
â”œâ”€â”€ requirement.txt           # Python dependencies
```

---

## âš™ï¸ Tech Stack

**Apache Spark 3.5 | PostgreSQL 15 | FastAPI | Docker**

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/<your-username>/retail-pipeline.git
cd retail-pipeline
```

### 2ï¸âƒ£ Environment Setup

Create a file named `.env` in the root directory with your database credentials:

```env
DB_USER=retail_user
DB_PASS=retail_pass
DB_NAME=retail_db
DB_PORT=5433   # default mapped port
```

### ğŸ”§ Access & Output

- **API Docs**: [http://localhost:8000](http://localhost:8000)
- **Postgres** â†’ Host: `localhost`, Port: `5433`

## ğŸ“Š Example Output

**Aggregated Sales (per city & product):**

| StoreCity | ProductID | Total_Sales | Avg_Sales |
| --------- | --------- | ----------- | --------- |
| Delhi     | P123      | 150000.00   | 250.50    |
| Jaipur    | P456      | 78000.00    | 180.30    |
| Mumbai    | P789      | 210500.00   | 310.75    |

---

## ğŸ“Œ Notes & Future Work

- The pipeline uses atomic writes to a current directory to prevent Spark overwrite errors.
- To completely reset the database and volumes, run docker-compose down -v.

**Future plans include**: adding multi-tenancy, expanding API endpoints with more analytics, and building a dashboard UI.
